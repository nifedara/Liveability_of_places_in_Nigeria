{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc7b87d",
   "metadata": {},
   "source": [
    "Question: Where are the worst and most liveable places in Nigeria?\n",
    "\n",
    "to answer this, I will:\n",
    "analyse a rich data about Nigeria\n",
    "analyse what was said about those places\n",
    "\n",
    "the data to be analyzed is wikipedia articles gotten from the wikipedia dump.\n",
    "Wikipedia, because it contains a lot of information about different places "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d02feab",
   "metadata": {},
   "source": [
    "1. Getting the data from the Wikipedia dump, parsing it and saving only articles about...\n",
    "\n",
    "2. Merging the articles together in one csv file\n",
    "The articles were in json format. so next step is to convert it to csv.\n",
    "what is now loaded here is the coverted file. Now in csv format, ready to be loaded into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a2a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f7f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is now loaded here is the csv file that contains articles that mention Nigeria\n",
    "#a total of 55474 rows and 3 columns: 'id', 'title', 'text'\n",
    "\n",
    "\n",
    "wikidump = pd.read_csv(r'Desktop\\mergedDump.csv')\n",
    "wikidump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd93e4fd",
   "metadata": {},
   "source": [
    "3. Getting the location in the title column of all the rows. So I can know what exact location the article is about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b4efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using spacy to get named entities in the title column\n",
    "#the found locations are set in a new column 'location'\n",
    "\n",
    "wikidump['text'] = wikidump['text'].astype(str) #cast type so the below function doesn't encounter any error. as spacy works\n",
    "                                                #on only string or Doc\n",
    "\n",
    "def get_loc(text):\n",
    "    doc = nlp(text)\n",
    "    results = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'LOC']]\n",
    "    return results\n",
    "\n",
    "wikidump[\"location\"] = wikidump['title'].apply(get_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6b8ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wrote the result to a new csv to save the time of running entity recognition everytime\n",
    "\n",
    "wikidump = pd.read_csv(r'Desktop\\wikidump-with-location.csv', index_col = 0)\n",
    "\n",
    "\n",
    "wikidump[\"location\"] = wikidump[\"location\"].apply(eval)  #this removes the quotation marks on the items in the location list\n",
    "\n",
    "wikidump['length'] = wikidump.location.apply(len) #to get the number of locations in each row of the new location column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove rows where the location is more or less than 1. so as to get rows(articles) that specially describe just one place\n",
    "\n",
    "wikidump = wikidump.loc[wikidump['length'] == 1] \n",
    "wikidump\n",
    "\n",
    "del wikidump['length'] #delete the length column. It is no longer needed as the filtering has now been done\n",
    "\n",
    "wikidump.reset_index(drop=True)\n",
    "\n",
    "\n",
    "#there are now 8444 rows × 4 columns left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca2ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidump = wikidump[wikidump[\"title\"].str.contains(\"Category:\") == False] #cleaning some irrelevant rows\n",
    "wikidump.reset_index(drop=True)\n",
    "\n",
    "#there are now 5081 rows × 4 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de94f723",
   "metadata": {},
   "source": [
    "4. Tokenizing the sentences in the articles. ie the 'text' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1886294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "def sent_token(text):\n",
    "    doc = nlp(text)\n",
    "    results= [sent.text for sent in doc.sents]\n",
    "    return results\n",
    "\n",
    "wikidump['sentences'] = wikidump['text'].astype(str).apply(sent_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wrote the result to a new csv to save the time of running sentence tokenization everytime\n",
    "\n",
    "wikidump = pd.read_csv(r'Documents\\wiki_sent_token.csv', index_col = 0)\n",
    "\n",
    "wikidump[\"sentences\"] = wikidump[\"sentences\"].apply(eval) #this removes the quotation marks on the items in the sentences list\n",
    "\n",
    "\n",
    "wikidump = wikidump.explode('sentences') #separate the sentences into diferent rows\n",
    "wikidump = wikidump.reset_index(drop=True)\n",
    "\n",
    "#there are now 481795 rows × 5 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fa3ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some extra cleaning, removing 'irrelevant' rows\n",
    "\n",
    "wikidump = wikidump[wikidump[\"sentences\"].str.contains(\"==\") == False].reset_index(drop=True) \n",
    "wikidump = wikidump[wikidump[\"sentences\"].str.contains(\"\\* \") == False].reset_index(drop=True)\n",
    "\n",
    "#there are now 358647 rows × 5 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c40cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del wiki['location']  #delete this location column. It is no longer needed\n",
    "wiki.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da4d782",
   "metadata": {},
   "source": [
    "5. Getting the location in each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5cdb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki['sentences'] = wiki['sentences'].astype(str)\n",
    "\n",
    "def get_loc(text):\n",
    "    doc = nlp(text)\n",
    "    results = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'LOC']]\n",
    "    return results\n",
    "\n",
    "wiki[\"location\"] = wiki['sentences'].apply(get_loc)\n",
    "\n",
    "\n",
    "#written to \n",
    "wiki = pd.read_csv(r'Documents\\wikipedia.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
